<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mukharbek Organokov - Senior AI Stack Engineer</title>
    <link rel="stylesheet" href="/src/css/style.css">
    <!-- <style>
        ... or might paste style.css content here
    </style> -->
</head>

<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="logo">Mukharbek Organokov</div>
            <ul class="nav-links">
                <li><a href="#home">Home</a></li>
                <li><a href="#about">About</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#blog">Blog</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <section id="home" class="hero">
        <div class="hero-content">
            <h1>Mukharbek Organokov</h1>
            <p>Senior AI Stack Engineer</p>
            <a href="#about" class="cta-button">Discover My Work</a>
        </div>
    </section>

    <!-- About Section -->
    <section id="about" class="about">
        <div class="container">
            <h2 class="section-title fade-in">About Me</h2>
            <div class="about-content">
                <div class="about-text fade-in">
                    <p class="spaced-paragraph">
                        I'm an AI/ML Engineer with a strong background in MLOps and experience delivering
                        production-grade ML systems. I've designed and deployed AI solutions for cybersecurity,
                        financial security, network intelligence, and large-scale data analytics, while
                        contributing to publications, patents, and mentoring AI talents.
                    </p>

                    <p class="spaced-paragraph">
                        I believe AI must be <strong>responsible</strong>, <strong>resilient</strong>,
                        and <strong>sustainable</strong>. My goal is to design robust, adaptive, and climate-aware
                        infrastructure — minimizing wasted power and compute, enabling scalable, trustworthy,
                        and energy-efficient innovation for a sustainable future.
                    </p>

                    <p>
                        Beyond industry, I co-founded Circassian DNA, a non-profit initiative reconnecting
                        the global Circassian diaspora through data-driven genealogy and genetic insights. I see
                        the power of AI not only to solve complex challenges, but also for fostering
                        human connection, preserving cultural heritage, and driving meaningful global impact.
                    </p>
                </div>
                <div class="skills fade-in">

                    <div class="skill-card">
                        <h4>Programming</h4>
                        <p>Python, Rust, SQL, Bash</p>
                    </div>

                    <div class="skill-card">
                        <h4>MLOps & Infra</h4>
                        <p>Docker, Kubernetes, Kubeflow, MLFlow, KServe, AWS, GitOps, ArgoCD, Helm, FastAPI, Flask</p>
                    </div>

                    <div class="skill-card">
                        <h4>Machine Learning</h4>
                        <p>XGBoost, LightGBM, Ray, Scikit-learn, ONNX</p>
                    </div>

                    <div class="skill-card">
                        <h4>Deep Learning</h4>
                        <p>PyTorch, TensorFlow, Keras, Transformers, GNN, GRU</p>
                    </div>

                    <div class="skill-card">
                        <h4>Gen AI</h4>
                        <p>LLM, Hugging Face, LangChain, LlamaIndex, Pinecone, Fine-tuning</p>
                    </div>

                    <div class="skill-card">
                        <h4>Data & Analytics</h4>
                        <p>Pandas, NumPy, SciPy, Matplotlib, Seaborn, Plotly</p>
                    </div>

                </div>
            </div>
        </div>
    </section>

    <!-- Experience Section -->
    <section id="experience">
        <div class="container">
            <h2 class="section-title fade-in">Career Journey</h2>
            <div class="timeline">
                <div class="timeline-item fade-in">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-date">2025 - Present</div>
                        <h3 class="timeline-title">Senior AI Stack Engineer</h3>
                        <div class="timeline-company">Stealth</div>
                        <p>
                            Building robust and scalable AI infrastructure to accelerate training, fine-tuning,
                            and deployment—while reducing cost, eliminating bottlenecks, and ensuring zero downtime.
                        </p>
                    </div>
                </div>

                <div class="timeline-item fade-in">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-date">2023 - 2025</div>
                        <h3 class="timeline-title">Machine Learning Engineer</h3>
                        <div class="timeline-company">Cisco Systems</div>
                        <p>
                            Designed and optimized scalable MLOps infrastructurefor seamless deployment, monitoring, and
                            lifecycle management of ML systems. Developed AI models for cybersecurity threat detection
                            and automated network configuration management. Architeced multi-agent LLMs systems.
                            Supervised a PhD intern and co-authored a publications, and secured a patent in applied AI.
                        </p>
                    </div>
                </div>

                <div class="timeline-item fade-in">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-date">2022 - 2023</div>
                        <h3 class="timeline-title">MLOps Engineer</h3>
                        <div class="timeline-company">StartupAI</div>
                        <p>
                            Designed and implemented MLOps benchmarking tools - leveraging experiment tracking and
                            pipeline cataloging — to evaluate and optimize ML workflows across internet security, brand
                            safety, digital media trust, and contextual Ad targeting.
                        </p>
                    </div>
                </div>

                <div class="timeline-item fade-in">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-date">2020 - 2022</div>
                        <h3 class="timeline-title">Data Scientist</h3>
                        <div class="timeline-company">sense4data</div>
                        <p>
                            Engineered and deployed end-to-end ML solutions - from financial security and fraud
                            detection to sports forecasting, computer vision ailway tie defect detection,
                            and an NLP system for resume-job offer matching. Enhanced anomaly detection in
                            transactional data and managed the full pipeline: data preprocessing, feature engineering,
                            model training, evaluation, and production deployment.
                        </p>
                    </div>
                </div>

                <div class="timeline-item fade-in">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-date">2019 - 2019</div>
                        <h3 class="timeline-title">Data Scientist</h3>
                        <div class="timeline-company">The Alan Turing Institute</div>
                        <p>
                            Contributed to applied research projects: shallow gas detection from seabed seismic imagery
                            (BGS); multi-sensor intelligent machining process monitoring (AMRC, co-facilitator);
                            semantic and instance segmentation of 3D point clouds (SenSat), analysis of network
                            metrics on customer perception of reliability (Telus).
                        </p>
                    </div>
                </div>

                <div class="timeline-item fade-in">
                    <div class="timeline-dot"></div>
                    <div class="timeline-content">
                        <div class="timeline-date">2016 - 2019</div>
                        <h3 class="timeline-title">PhD Researcher</h3>
                        <div class="timeline-company">CNRS</div>
                        <p>
                            Pursued doctoral research in multi-messenger astrophysics, investigating the joint detection
                            of neutrinos and gamma rays through experiments such as ANTARES, HAWC, and KM3NeT.
                            Focused on data analysis, detector performance optimization, and the development of
                            advanced methodologies for high-energy particle detection.
                        </p>
                    </div>
                </div>


            </div>
        </div>
    </section>

    <!-- Blog Section -->
    <section id="blog" class="about">
        <div class="container">
            <h2 class="section-title fade-in">Latest Blog Posts</h2>
            <div class="blog-grid">

                <article class="blog-card fade-in">
                    <div class="blog-image">🤖</div>
                    <div class="blog-content">
                        <h3 class="blog-title">The Future of AGI: Challenges and Opportunities</h3>
                        <p class="blog-excerpt">
                            Explore the evolving landscape of artificial general intelligence — highlighting how
                            existing systems excel at narrow tasks, while AGI aspires to human-like reasoning,
                            adaptability, and cross-domain learning. AGI promises transformative breakthroughs
                            in domains such as healthcare, scientific discovery, and personalized education,
                            even as profound challenges remain in areas like real-time learning,
                            ethical alignment, and safe deployment.
                            ... in progress
                        </p>
                        <div class="blog-meta">
                            <span>Sep 15, 2025</span>
                            <span>10 min read</span>
                        </div>
                    </div>
                </article>

                <article class="blog-card fade-in">
                    <div class="blog-image">🔬</div>
                    <div class="blog-content">
                        <h3 class="blog-title">Ethical AI: Bias Detection in Deep Learning</h3>
                        <p class="blog-excerpt">
                            Understanding and mitigating bias in deep learning is fundamental to building fair,
                            inclusive AI systems. Bias can manifest at different stages—unbalanced class distributions,
                            misrepresentative training data, and flawed model assumptions. Common mitigation strategies
                            include **pre-processing** (e.g., re-sampling or re-weighting data), **in-processing**
                            (e.g., fairness-aware training and adversarial debiasing), and **post-processing**
                            adjustments to outputs. Regular auditing, monitoring, and explainability are also vital for
                            maintaining fairness over time, supported by tools like AIF360 and fairness dashboards.
                            ... in progress
                        </p>

                        <div class="blog-meta">
                            <span>Aug 31, 2025</span>
                            <span>10 min read</span>
                        </div>
                    </div>
                </article>

                <article class="blog-card fade-in">
                    <div class="blog-image">🧠</div>
                    <div class="blog-content">
                        <h3 class="blog-title">Circassian DNA Chatbot</h3>
                        <p class="blog-excerpt">
                            A hands-on guide to building a production-ready chatbot on the Circassian DNA
                            project—illustrating how robust MLOps pipelines power each stage from training to
                            deployment. Learn how we streamlined model management, versioning, and continuous
                            integration for real-time, culturally-aware interactions.
                            ... in progress
                        </p>
                        <div class="blog-meta">
                            <span>Aug 30, 2025</span>
                            <span>15 min read</span>
                        </div>
                    </div>
                </article>

                <article class="blog-card fade-in">
                    <div class="blog-image">⚡</div>
                    <div class="blog-content">
                        <h3 class="blog-title">Ultimate Throughput with vLLM</h3>
                        <p class="blog-excerpt">

                            Discover how <a href="https://docs.vllm.ai/en/latest/" target="_blank">vLLM</a>—powered by
                            its virtual memory-inspired <a href="https://arxiv.org/abs/2501.01005"
                                target="_blank">PagedAttention</a> engine — significantly reduces memory fragmentation
                            in <a
                                href="https://medium.com/%40tam.tamanna18/vllm-and-tools-for-optimizing-large-language-model-performance-c4b7a1273bee"
                                target="_blank">KV caches</a>, enabling near-zero waste and up to a <a
                                href="https://medium.com/%40christopher.keibel.90/why-you-might-want-to-use-vllm-for-your-open-source-llm-serving-5c2836432e81"
                                target="_blank">2-4 times</a> throughput improvement over SOTA inference engines like
                            FasterTransformer, especially with longer contexts and larger models, making it the new
                            standard for high-performance LLM serving. vLLM inherently uses optimized CUDA-backed
                            kernels—such as <a href="https://arxiv.org/abs/2205.14135" target="_blank">
                                FlashAttention</a> and <a href="https://arxiv.org/abs/2501.01005"
                                target="_blank">FlashInfer</a> — to achieve top-tier <a
                                href="https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html"
                                target="_blank">GPU</a> utilization without requiring teams to write custom kernels.
                            FlashInfer delivers substantial latency improvements for GPU-serving LLMs—including a 29-69%
                            reduction in inter-token latency compared to Triton backends, 28-30% lower latency for long
                            context inference, and a 13-17% performance boost in parallel generation scenarios.

                            vLLM runs standard Hugging Face models, integrates seamlessly with PyTorch, and supports
                            OpenAI-compatible APIs — so you don't need to rebuild; just swap in a better engine. <a
                                href="https://docs.vllm.ai/en/v0.8.5/design/huggingface_integration.html"
                                target="_blank">Hugging Face integration</a> confirms that vLLM can run standard HF
                            models without rewriting your workflows. <a
                                href="https://pytorch.org/blog/vllm-joins-pytorch/" target="_blank">PyTorch ecosystem
                                integration</a> shows it fits seamlessly into the PyTorch ecosystem and
                            supports diverse hardware backends with minimal hassle. And a built-in <a
                                href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html"
                                target="_blank">OpenAI-compatible server</a> demonstrates vLLM's ability to mimic the
                            OpenAI API, enabling plug-and-play usage in existing setups.

                            In just half a year, vLLM accelerated from 30K to 50K stars on GitHub — highlighting its
                            fast-moving momentum, robust open-source growth, and deep support from the developer
                            community.

                            ... in progress
                        </p>
                        <div class="blog-meta">
                            <span>Aug 25, 2025</span>
                            <span>15 min read</span>
                        </div>
                    </div>
                </article>

                <article class="blog-card fade-in">
                    <div class="blog-image">🚀</div>
                    <div class="blog-content">
                        <h3 class="blog-title">KServe Meets vLLM: Cloud-Native Scalable LLM Serving on Kubernetes</h3>
                        <p class="blog-excerpt">

                            <a href="https://kserve.github.io/website/" target="_blank">KServe</a> is a
                            Kubernetes-native model serving platform offering enterprise-grade capabilities
                            for predictive and generative AI—including autoscaling, ModelMesh routing, and standardized
                            APIs across Hugging Face and <a href="https://docs.vllm.ai/en/latest/"
                                target="_blank">vLLM</a> models. Since the <a
                                href="https://kserve.github.io/website/0.13/blog/articles/2024-05-15-KServe-0.13-release/"
                                target="_blank">v0.13 release</a>, KServe officially supports vLLM as a backend runtime,
                            enabling high-performance LLM serving within the Kubernetes ecosystem. With the <a
                                href="https://www.cncf.io/blog/2025/06/18/announcing-kserve-v0-15-advancing-generative-ai-model-serving/"
                                target="_blank">v0.15 release</a>, KServe added advanced LLM-specific features including
                            distributed KV caching via <a href="https://lmcache.ai/" target="_blank">LMCache</a>,
                            LLM-aware autoscaling using KEDA, and integration with
                            the Envoy AI Gateway for intelligent routing and traffic management.

                            KServe supports a plug-and-play architecture using custom ServingRuntimes and
                            InferenceService CRDs, enabling optimized LLM serving via vLLM backends across GPUs (NVIDIA,
                            AMD, Intel) and CPUs. Seamless integration matters - leverage vLLM's high-efficiency
                            engine without rebuilding your stack while KServe handles orchestration, scaling, and
                            routing. Features like LMCache maximize GPU throughput critical for LLMs. Offloading of
                            KV-cache layers reduces inference costs and helps meet SLOs for latency and throughput.

                            ... in progress
                        </p>
                        <div class="blog-meta">
                            <span>Aug 22, 2025</span>
                            <span>15 min read</span>
                        </div>
                    </div>
                </article>


                <article class="blog-card fade-in">
                    <div class="blog-image">☁️</div>
                    <div class="blog-content">
                        <h3 class="blog-title">Kubeflow: Cloud-Native MLOps on Kubernetes for Scalable ML Workflows</h3>
                        <p class="blog-excerpt">

                            <a href="https://www.kubeflow.org/" target="_blank">Kubeflow</a> is a modular, open-source
                            MLOps platform built on Kubernetes, designed to orchestrate the entire machine learning
                            lifecycle — from notebooks and pipeline orchestration to distributed training,
                            hyperparameter tuning, and model serving via <a href="https://kserve.github.io/website/"
                                target="_blank">KServe</a>. Its modular architecture allows you to adopt only the
                            components you need. Kubeflow <a href="https://www.kubeflow.org/docs/components/"
                                target="_blank">supports</a> Notebooks, Pipelines, Training Operators, Katib,
                            and KServe — covering all stages from experimentation to deployment. <a
                                href="https://www.cncf.io/blog/2025/06/18/announcing-kserve-v0-15-advancing-generative-ai-model-serving/"
                                target="_blank">Kubeflow Pipelines</a> enables building portable, container-based
                            workflows deployable across any Kubernetes environment.

                            As a CNCF-incubated project backed by Google, Red Hat, IBM, NVIDIA, and others, Kubeflow
                            delivers enterprise-grade scalability, hybrid-cloud portability, and robust governance.

                            For teams with Kubernetes-first infrastructure, Kubeflow offers a production-ready
                            foundation for reproducible, scalable ML workflows — without stitching together disparate
                            tools. Moreover, Kubeflow's tight integration with Kubernetes enables efficient resource
                            management—especially for GPU workloads—making it ideal for large-scale ML operations.

                            For ML teams focused mainly on experiment tracking, model versioning, and ease-of-use,
                            <a href="https://mlflow.org/" target="_blank">MLflow</a> is a <a
                                href="https://www.zenml.io/blog/kubeflow-vs-mlflow" target="_blank">great MLOps
                                solution</a>.

                            ... in progress
                        </p>
                        <div class="blog-meta">
                            <span>Aug 20, 2025</span>
                            <span>15 min read</span>
                        </div>
                    </div>
                </article>


            </div>
        </div>
    </section>

    <!-- Contact Section -->
    <section id="contact" class="contact">
        <div class="container">
            <h2 class="section-title fade-in" style="color: white;">Get In Touch</h2>
            <div class="contact-content">
                <div class="contact-info fade-in">
                    <h3>Let's Connect</h3>
                    <p>
                        I'm always interested in discussing AI, progress with AGI, LLM architectures,
                        MLOPs tools, machine learning algorithms, and potential collaboration opportunities.
                    </p>

                    <div class="contact-item">
                        <span>📧</span>
                        <span>mukharbek.organokov@gmail.com</span>
                    </div>
                    <div class="contact-item">
                        <span>💼</span>
                        <span>https://www.linkedin.com/in/circassia/</span>
                    </div>
                    <div class="contact-item">
                        <span>🐙</span>
                        <span>https://github.com/kabartay</span>
                    </div>
                    <div class="contact-item">
                        <span>📍</span>
                        <span>Paris, FR</span>
                    </div>
                </div>

                <form class="contact-form fade-in">
                    <div class="form-group">
                        <label for="name">Name</label>
                        <input type="text" id="name" placeholder="Your Name">
                    </div>
                    <div class="form-group">
                        <label for="email">Email</label>
                        <input type="email" id="email" placeholder="your.email@example.com">
                    </div>
                    <div class="form-group">
                        <label for="message">Message</label>
                        <textarea id="message" rows="5" placeholder="Your message..."></textarea>
                    </div>
                    <button type="submit" class="submit-btn">Send Message</button>
                </form>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Mukharbek Organokov. All rights reserved. Built with passion for AI.</p>
        </div>
    </footer>


    <script type="module" src="/src/js/main.js"></script>
    <!-- <script>
        ... or might paste main.js content here
    </script> -->
</body>

</html>